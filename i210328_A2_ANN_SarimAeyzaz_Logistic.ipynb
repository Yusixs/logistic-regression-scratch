{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ANN Assignment 2: Logistic Regression\n",
        "## Submitted By: Sarim Aeyzaz (i21-0328)"
      ],
      "metadata": {
        "id": "B7sOpzAXoQ-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MguP5Lc-BMNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "DjfMmroKBUoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, Y, split = 0.25):\n",
        "\n",
        "  length = X.shape[0]\n",
        "  index_split = int(length * split)\n",
        "\n",
        "  # Generates random order of numbers from 0 till length\n",
        "  indices = np.random.permutation(length)\n",
        "  test_indices, train_indices = indices[:index_split], indices[index_split:]\n",
        "\n",
        "  X_train, X_test = X[train_indices], X[test_indices]\n",
        "  Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
        "  return X_train, X_test, Y_train, Y_test"
      ],
      "metadata": {
        "id": "5tbwGrlBkLl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(X, Y):\n",
        "  return (X == Y).sum() / len(Y) * 100"
      ],
      "metadata": {
        "id": "hGV2nKLMnDLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self):\n",
        "    self.thetas = None\n",
        "    self.lr = None\n",
        "\n",
        "  def __sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  # Loss function\n",
        "  def __cost(self, predictions, y):\n",
        "    # Logistic Formula: -1/(no. of features) * [ (y)*log(h) + (1-y)*log(1-h) ]\n",
        "    cost = -(1 / len(y)) * (np.sum(y.T.dot(np.log(predictions)) + (1 - y).T.dot(np.log(1 - predictions))))\n",
        "    return cost\n",
        "\n",
        "  # Calculates partial derrivative of MSE w.r.t every X value\n",
        "  def __derrivative(self, X, y, predictions): # This function calculates the theta value by gradient descent\n",
        "    # Formula below is basically: [ (features + 1, samples) * (samples, 1) ] / no. of features\n",
        "    return np.dot(X.T, (y - predictions)) / X.shape[0] # Returns a (features + 1, 1) shape array\n",
        "\n",
        "  # Update theta values based on partial derrivate error\n",
        "  def __gradient_ascent(self, lr, theta, del_theta):\n",
        "    return theta + lr * del_theta\n",
        "\n",
        "  # Predict values given a feature vector\n",
        "  def __predict(self, X, theta):\n",
        "    answer = np.dot(X, theta) # (samples, features + 1) * (features + 1, 1)\n",
        "    return self.__sigmoid(answer) # (samples, 1)\n",
        "\n",
        "  # 2. Predict_Class() function which accepts X as argument and returns classes for those test examples\n",
        "  # 3. Predict_Confidence() function which accepts X as argument and returns the probabilities for those test examples\n",
        "\n",
        "  # Return weights (thetas) of the model\n",
        "  def Get_Weights(self):\n",
        "    return self.thetas\n",
        "\n",
        "  def Predict_Confidence(self, X):\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1)))).T\n",
        "    predictions = self.Get_Weights().dot(X)\n",
        "    return self.__sigmoid(predictions)\n",
        "\n",
        "  def Predict_Class(self, X):\n",
        "    X = self.Predict_Confidence(X)\n",
        "    X[X > 0.5], X[X <= 0.5] = 1, 0\n",
        "    return X\n",
        "\n",
        "  def Train(self, X, Y, alpha = 0.0001, loss_at_iter = 50, max_iter = None):\n",
        "\n",
        "    # Setting up some stuff\n",
        "    convergence_check = False\n",
        "    X = np.array(X) # Dimensions are: (samples, features)\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1)))) # Dimensions are (samples, features + 1)\n",
        "    Y = np.array(Y).reshape(-1, 1) # Dimensions are: (samples, 1)\n",
        "\n",
        "    self.thetas = np.empty((0,X.shape[1]), float)\n",
        "\n",
        "    # Handling Termination by changes in old and new loss values incase max_iter is not defined\n",
        "    if max_iter is None:\n",
        "      convergence_check = True\n",
        "      print(\"Convergence Criteria will be used\")\n",
        "\n",
        "    old_loss = 0\n",
        "    counter = 0\n",
        "\n",
        "    # Generating random weights\n",
        "    theta = np.random.random(size=(X.shape[1], 1))  # Dimensions of theta = (features + 1, 1)\n",
        "\n",
        "    while(True):\n",
        "\n",
        "      predictions = self.__predict(X, theta) # Returns (samples, 1)\n",
        "\n",
        "      loss = self.__cost(predictions, Y) # Returns Integer\n",
        "\n",
        "      if (counter % loss_at_iter == 0):\n",
        "          print(f\"Loss at iteration {counter} = {loss}\")\n",
        "\n",
        "      del_theta = self.__derrivative(X, Y, predictions) # (features + 1, 1)\n",
        "\n",
        "      theta = self.__gradient_ascent(alpha, theta, del_theta)\n",
        "\n",
        "      # Either do convergence check or max iteration check\n",
        "      if convergence_check:\n",
        "        if abs(old_loss - loss) / loss < 0.00001: # If the loss difference is lesser than 0.01%, break\n",
        "          print(\"Convergence Reached\")\n",
        "          break\n",
        "      else:\n",
        "        if (counter >= max_iter - 1): # If max iterations are reached, break\n",
        "          print(\"Maximum Iterations Reached\")\n",
        "          break\n",
        "\n",
        "      old_loss = loss\n",
        "      counter +=1\n",
        "\n",
        "    print(f\"Final Loss at iteration {counter} = {loss}\\n\")\n",
        "\n",
        "    self.thetas = np.vstack((self.thetas, theta.reshape(1, -1)))\n",
        "\n"
      ],
      "metadata": {
        "id": "2mz8nvHQBYBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, Y_names = iris['data'], iris['target'], iris['target_names']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 0.1)\n",
        "\n",
        "setosa = np.where(Y_train == 0, 1, 0)\n",
        "versicolor = np.where(Y_train == 1, 1, 0)\n",
        "virginica = np.where(Y_train == 2, 1, 0)"
      ],
      "metadata": {
        "id": "MdnlYk7Rkpqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_setosa = LogisticRegression()\n",
        "LR_versicolor = LogisticRegression()\n",
        "LR_virginica = LogisticRegression()\n",
        "\n",
        "LR_setosa.Train(X_train, setosa, 0.01, 1000)\n",
        "LR_versicolor.Train(X_train, versicolor, 0.01, 1000, 10000)\n",
        "LR_virginica.Train(X_train, virginica, 0.01, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so0UwgXQBZWI",
        "outputId": "290128ef-c50e-4c80-c7eb-dea8ebae011e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence Criteria will be used\n",
            "Loss at iteration 0 = 3.1762759603046917\n",
            "Loss at iteration 1000 = 0.06016017501183288\n",
            "Loss at iteration 2000 = 0.031389137784427486\n",
            "Loss at iteration 3000 = 0.02157613161334905\n",
            "Loss at iteration 4000 = 0.01657123087543353\n",
            "Loss at iteration 5000 = 0.013516364801552503\n",
            "Loss at iteration 6000 = 0.01144914305441789\n",
            "Loss at iteration 7000 = 0.009952968528216885\n",
            "Loss at iteration 8000 = 0.00881755274897843\n",
            "Loss at iteration 9000 = 0.007925021129378738\n",
            "Loss at iteration 10000 = 0.007204055604188129\n",
            "Loss at iteration 11000 = 0.006608915661971542\n",
            "Loss at iteration 12000 = 0.0061088831795028786\n",
            "Loss at iteration 13000 = 0.005682544631379827\n",
            "Loss at iteration 14000 = 0.005314504904932754\n",
            "Loss at iteration 15000 = 0.004993404678573672\n",
            "Loss at iteration 16000 = 0.004710674716644796\n",
            "Loss at iteration 17000 = 0.004459725732268341\n",
            "Loss at iteration 18000 = 0.004235405666311099\n",
            "Loss at iteration 19000 = 0.004033626616517461\n",
            "Loss at iteration 20000 = 0.0038511025151067306\n",
            "Loss at iteration 21000 = 0.0036851609461047204\n",
            "Loss at iteration 22000 = 0.0035336057157331216\n",
            "Loss at iteration 23000 = 0.003394614866450256\n",
            "Loss at iteration 24000 = 0.0032666638912504397\n",
            "Loss at iteration 25000 = 0.0031484671581432917\n",
            "Loss at iteration 26000 = 0.003038932688960993\n",
            "Loss at iteration 27000 = 0.0029371268640526345\n",
            "Loss at iteration 28000 = 0.002842246596069812\n",
            "Loss at iteration 29000 = 0.0027535971881967256\n",
            "Loss at iteration 30000 = 0.0026705745640855988\n",
            "Loss at iteration 31000 = 0.0025926508926324876\n",
            "Loss at iteration 32000 = 0.002519362872822492\n",
            "Loss at iteration 33000 = 0.002450302120427966\n",
            "Loss at iteration 34000 = 0.002385107228512963\n",
            "Loss at iteration 35000 = 0.0023234571706490415\n",
            "Loss at iteration 36000 = 0.002265065788649294\n",
            "Loss at iteration 37000 = 0.002209677161934248\n",
            "Loss at iteration 38000 = 0.0021570616979553667\n",
            "Loss at iteration 39000 = 0.0021070128157275898\n",
            "Loss at iteration 40000 = 0.0020593441198674895\n",
            "Loss at iteration 41000 = 0.0020138869823599494\n",
            "Loss at iteration 42000 = 0.0019704884648894774\n",
            "Loss at iteration 43000 = 0.001929009526945906\n",
            "Loss at iteration 44000 = 0.0018893234747763116\n",
            "Loss at iteration 45000 = 0.001851314614163343\n",
            "Loss at iteration 46000 = 0.001814877076383453\n",
            "Loss at iteration 47000 = 0.0017799137918620633\n",
            "Loss at iteration 48000 = 0.0017463355902466798\n",
            "Loss at iteration 49000 = 0.0017140604090577444\n",
            "Loss at iteration 50000 = 0.0016830125959014417\n",
            "Loss at iteration 51000 = 0.0016531222915605742\n",
            "Loss at iteration 52000 = 0.0016243248832097782\n",
            "Loss at iteration 53000 = 0.0015965605186080848\n",
            "Loss at iteration 54000 = 0.0015697736734626108\n",
            "Loss at iteration 55000 = 0.0015439127652804674\n",
            "Loss at iteration 56000 = 0.00151892980797071\n",
            "Loss at iteration 57000 = 0.0014947801022546702\n",
            "Loss at iteration 58000 = 0.0014714219576177031\n",
            "Loss at iteration 59000 = 0.0014488164421073123\n",
            "Loss at iteration 60000 = 0.00142692715677028\n",
            "Loss at iteration 61000 = 0.0014057200319374107\n",
            "Loss at iteration 62000 = 0.0013851631429206367\n",
            "Loss at iteration 63000 = 0.0013652265429932925\n",
            "Loss at iteration 64000 = 0.001345882111787484\n",
            "Loss at iteration 65000 = 0.0013271034174697303\n",
            "Loss at iteration 66000 = 0.0013088655912522093\n",
            "Loss at iteration 67000 = 0.0012911452129678288\n",
            "Loss at iteration 68000 = 0.0012739202065847532\n",
            "Loss at iteration 69000 = 0.0012571697446653266\n",
            "Loss at iteration 70000 = 0.0012408741608864026\n",
            "Loss at iteration 71000 = 0.0012250148698369867\n",
            "Loss at iteration 72000 = 0.0012095742933948862\n",
            "Loss at iteration 73000 = 0.0011945357930595222\n",
            "Loss at iteration 74000 = 0.0011798836076852702\n",
            "Loss at iteration 75000 = 0.0011656027961176748\n",
            "Loss at iteration 76000 = 0.001151679184287094\n",
            "Loss at iteration 77000 = 0.001138099316359891\n",
            "Loss at iteration 78000 = 0.0011248504095876539\n",
            "Loss at iteration 79000 = 0.0011119203125315254\n",
            "Loss at iteration 80000 = 0.0010992974663698851\n",
            "Loss at iteration 81000 = 0.0010869708690267419\n",
            "Loss at iteration 82000 = 0.0010749300418830196\n",
            "Loss at iteration 83000 = 0.001063164998856057\n",
            "Loss at iteration 84000 = 0.0010516662176524817\n",
            "Loss at iteration 85000 = 0.001040424613017903\n",
            "Loss at iteration 86000 = 0.0010294315118230112\n",
            "Loss at iteration 87000 = 0.001018678629840558\n",
            "Loss at iteration 88000 = 0.001008158050080193\n",
            "Loss at iteration 89000 = 0.0009978622025605115\n",
            "Loss at iteration 90000 = 0.000987783845408048\n",
            "Convergence Reached\n",
            "Final Loss at iteration 90866 = 0.0009792263575555428\n",
            "\n",
            "Loss at iteration 0 = 5.632663122776098\n",
            "Loss at iteration 1000 = 0.5786873822481552\n",
            "Loss at iteration 2000 = 0.5639203478039064\n",
            "Loss at iteration 3000 = 0.5534893392837963\n",
            "Loss at iteration 4000 = 0.5459767116992053\n",
            "Loss at iteration 5000 = 0.5404592688424362\n",
            "Loss at iteration 6000 = 0.5363244893129746\n",
            "Loss at iteration 7000 = 0.5331621188242877\n",
            "Loss at iteration 8000 = 0.530693792797163\n",
            "Loss at iteration 9000 = 0.5287279550832479\n",
            "Maximum Iterations Reached\n",
            "Final Loss at iteration 9999 = 0.5271323758591171\n",
            "\n",
            "Convergence Criteria will be used\n",
            "Loss at iteration 0 = 2.635430089664264\n",
            "Loss at iteration 1000 = 0.2804879596610706\n",
            "Loss at iteration 2000 = 0.22956971613323712\n",
            "Loss at iteration 3000 = 0.2005484345855188\n",
            "Loss at iteration 4000 = 0.18094150191391864\n",
            "Loss at iteration 5000 = 0.16664102826943694\n",
            "Loss at iteration 6000 = 0.15569346257831718\n",
            "Loss at iteration 7000 = 0.14701502308579778\n",
            "Loss at iteration 8000 = 0.13994866042253604\n",
            "Loss at iteration 9000 = 0.13407093249169727\n",
            "Loss at iteration 10000 = 0.12909603264135003\n",
            "Loss at iteration 11000 = 0.1248238092204742\n",
            "Loss at iteration 12000 = 0.12110977804659968\n",
            "Loss at iteration 13000 = 0.11784694493466533\n",
            "Loss at iteration 14000 = 0.11495433198151912\n",
            "Loss at iteration 15000 = 0.11236948466561951\n",
            "Loss at iteration 16000 = 0.11004343389868328\n",
            "Loss at iteration 17000 = 0.10793722182367269\n",
            "Loss at iteration 18000 = 0.10601945209157267\n",
            "Loss at iteration 19000 = 0.10426452812365682\n",
            "Loss at iteration 20000 = 0.10265136364081358\n",
            "Loss at iteration 21000 = 0.10116242380712277\n",
            "Loss at iteration 22000 = 0.09978300194923306\n",
            "Loss at iteration 23000 = 0.09850066684020437\n",
            "Loss at iteration 24000 = 0.09730483528849881\n",
            "Loss at iteration 25000 = 0.09618643801533427\n",
            "Loss at iteration 26000 = 0.0951376558373025\n",
            "Loss at iteration 27000 = 0.09415170943244204\n",
            "Convergence Reached\n",
            "Final Loss at iteration 27325 = 0.09384379843859027\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Setosa weights = {LR_setosa.Get_Weights()}\\n\")\n",
        "print(f\"Versicolor weights = {LR_versicolor.Get_Weights()}\\n\")\n",
        "print(f\"Virginica weights = {LR_virginica.Get_Weights()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZCoxMYTDYra",
        "outputId": "f198ebcb-bde3-4dad-d5a8-1f500d0135e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setosa weights = [[ 0.5331376   3.04005205 -4.39432548 -1.26479666  0.66375441]]\n",
            "\n",
            "Versicolor weights = [[ 0.53638734 -1.7622374   0.48308176 -1.40709834  1.27049573]]\n",
            "\n",
            "Virginica weights = [[-2.92472044 -2.93299533  4.14883882  4.69431186 -1.43666984]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with np.printoptions(precision=5, suppress = True):\n",
        "  print(f\"Setosa Confidence = {LR_setosa.Predict_Confidence(X_test)}\")\n",
        "  print(f\"Versicolor Confidence = {LR_versicolor.Predict_Confidence(X_test)}\")\n",
        "  print(f\"Virginica Confidence = {LR_virginica.Predict_Confidence(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTGzJbytE5g-",
        "outputId": "6839b3bd-33e1-4688-91ea-c950570d4ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setosa Confidence = [[0.0043  0.99951 0.99939 0.00002 0.0001  0.      0.99965 0.00018 0.\n",
            "  0.99962 0.00069 0.98135 0.99534 0.00059 0.99997]]\n",
            "Versicolor Confidence = [[0.52158 0.14593 0.12244 0.36537 0.43494 0.21918 0.12897 0.56724 0.34629\n",
            "  0.11955 0.55104 0.45933 0.24485 0.51088 0.08556]]\n",
            "Virginica Confidence = [[0.01188 0.      0.      0.6415  0.17544 0.98757 0.      0.17214 0.88301\n",
            "  0.      0.02181 0.      0.      0.02194 0.     ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example belongs to Setosa Class = {LR_setosa.Predict_Class(X_test)}\")\n",
        "print(f\"Example belongs to Versicolor Class = {LR_versicolor.Predict_Class(X_test)}\")\n",
        "print(f\"Example belongs to Virginica Class = {LR_virginica.Predict_Class(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frWJ4vNdHUUN",
        "outputId": "b2a6e20c-8fff-4d42-c6bd-30d760b7fa30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example belongs to Setosa Class = [[0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.]]\n",
            "Example belongs to Versicolor Class = [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]]\n",
            "Example belongs to Virginica Class = [[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_setosa = LR_setosa.Predict_Confidence(X_test)\n",
        "labels_versicolor = LR_versicolor.Predict_Confidence(X_test)\n",
        "labels_virginica = LR_virginica.Predict_Confidence(X_test)\n",
        "\n",
        "# np.argmax(np.array((labels_setosa, labels_versicolor, labels_virginica)))\n",
        "combined = np.argmax(np.vstack((labels_setosa, labels_versicolor, labels_virginica)), axis = 0)\n",
        "print(combined)\n",
        "accuracy = calculate_accuracy(combined, Y_test)\n",
        "print(f\"Accuracy = {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGGYT4u2I1pa",
        "outputId": "54a9cbd7-07fe-4d7f-dd73-cc12baa29254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 0 1 1 2 0 0 0 0 1 1 2 2 2]\n",
            "Accuracy = 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Better implementation below (imo) but doesn't follow assignment guidelines exactly (but its generic and works on n classes)"
      ],
      "metadata": {
        "id": "CFR-VJLVBN7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fvtKWgdH-W-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X, Y, Y_names = iris['data'], iris['target'], iris['target_names']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 0.1)"
      ],
      "metadata": {
        "id": "NYR1LrF2RtRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpS1cOcgh9OS"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self):\n",
        "    self.thetas = None\n",
        "    self.lr = None\n",
        "\n",
        "  def __sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  # Loss function\n",
        "  def __cost(self, predictions, y):\n",
        "    # Logistic Formula: -1/(no. of features) * [ (y)*log(h) + (1-y)*log(1-h) ]\n",
        "    cost = -(1 / len(y)) * (np.sum(y.T.dot(np.log(predictions)) + (1 - y).T.dot(np.log(1 - predictions))))\n",
        "    return cost\n",
        "\n",
        "  # Calculates partial derrivative of MSE w.r.t every X value\n",
        "  def __derrivative(self, X, y, predictions): # This function calculates the theta value by gradient descent\n",
        "    # Formula below is basically: [ (features + 1, samples) * (samples, 1) ] / no. of features\n",
        "    return np.dot(X.T, (y - predictions)) / X.shape[0] # Returns a (features + 1, 1) shape array\n",
        "\n",
        "  # Update theta values based on partial derrivate error\n",
        "  def __gradient_ascent(self, lr, theta, del_theta):\n",
        "    return theta + lr * del_theta\n",
        "\n",
        "  # Predict values given a feature vector\n",
        "  def __predict(self, X, theta):\n",
        "    answer = np.dot(X, theta) # (samples, features + 1) * (features + 1, 1)\n",
        "    return self.__sigmoid(answer) # (samples, 1)\n",
        "\n",
        "  # 2. Predict_Class() function which accepts X as argument and returns classes for those test examples\n",
        "  # 3. Predict_Confidence() function which accepts X as argument and returns the probabilities for those test examples\n",
        "\n",
        "  # Return weights (thetas) of the model\n",
        "  def Get_Weights(self):\n",
        "    return self.thetas\n",
        "\n",
        "  def Predict_Confidence(self, X):\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1)))).T\n",
        "    predictions = self.Get_Weights().dot(X).T\n",
        "    return self.__sigmoid(predictions)\n",
        "\n",
        "  def Predict_Class(self, X):\n",
        "    return np.argmax(self.Predict_Confidence(X), axis = 1)\n",
        "\n",
        "  def Train(self, X, Y, alpha = 0.0001, loss_at_iter = 50, max_iter = None):\n",
        "\n",
        "    # Setting up some stuff\n",
        "    convergence_check = False\n",
        "    X = np.array(X) # Dimensions are: (samples, features)\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1)))) # Dimensions are (samples, features + 1)\n",
        "    Y = np.array(Y).reshape(-1, 1) # Dimensions are: (samples, 1)\n",
        "\n",
        "    self.thetas = np.empty((0,X.shape[1]), float)\n",
        "\n",
        "\n",
        "    # Handling Termination by changes in old and new loss values incase max_iter is not defined\n",
        "    if max_iter is None:\n",
        "      convergence_check = True\n",
        "      print(\"Convergence Criteria will be used\")\n",
        "\n",
        "    for i in np.unique(Y):\n",
        "\n",
        "\n",
        "      old_loss = 0\n",
        "      counter = 0\n",
        "\n",
        "      # Selecting Y value based on One vs All logic\n",
        "      Y_OVA = np.where(i == Y, 1, 0)\n",
        "\n",
        "      # Generating random weights\n",
        "      theta = np.random.random(size=(X.shape[1], 1))  # Dimensions of theta = (features + 1, 1)\n",
        "\n",
        "      while(True):\n",
        "\n",
        "        predictions = self.__predict(X, theta) # Returns (samples, 1)\n",
        "\n",
        "        loss = self.__cost(predictions, Y_OVA) # Returns Integer\n",
        "\n",
        "        if (counter % loss_at_iter == 0):\n",
        "            print(f\"Loss at iteration {counter} = {loss}\")\n",
        "\n",
        "        del_theta = self.__derrivative(X, Y_OVA, predictions) # (features + 1, 1)\n",
        "\n",
        "        theta = self.__gradient_ascent(alpha, theta, del_theta)\n",
        "\n",
        "        # Either do convergence check or max iteration check\n",
        "        if convergence_check:\n",
        "          if abs(old_loss - loss) / loss < 0.00001: # If the loss difference is lesser than 0.01%, break\n",
        "            print(\"Convergence Reached\")\n",
        "            break\n",
        "        else:\n",
        "          if (counter >= max_iter - 1): # If max iterations are reached, break\n",
        "            print(\"Maximum Iterations Reached\")\n",
        "            break\n",
        "\n",
        "        old_loss = loss\n",
        "        counter +=1\n",
        "\n",
        "      print(f\"Final Loss at iteration {counter} = {loss}\")\n",
        "      print(f\"Training Complete for {i}!\\n\")\n",
        "\n",
        "      self.thetas = np.vstack((self.thetas, theta.reshape(1, -1)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.Train(X_train, Y_train, 0.01, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zllDzKgXUqY",
        "outputId": "9b2c44f9-0e28-4c52-d086-a86a6ca52aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence Criteria will be used\n",
            "Loss at iteration 0 = 7.250516723372894\n",
            "Loss at iteration 1000 = 0.06125792571932941\n",
            "Loss at iteration 2000 = 0.032153444535820475\n",
            "Loss at iteration 3000 = 0.02216084824132923\n",
            "Loss at iteration 4000 = 0.01704940931178735\n",
            "Loss at iteration 5000 = 0.013924344654355044\n",
            "Loss at iteration 6000 = 0.011807327838349924\n",
            "Loss at iteration 7000 = 0.010273914142407938\n",
            "Loss at iteration 8000 = 0.009109530553878637\n",
            "Loss at iteration 9000 = 0.008193770722803037\n",
            "Loss at iteration 10000 = 0.007453725611274241\n",
            "Loss at iteration 11000 = 0.006842604764596379\n",
            "Loss at iteration 12000 = 0.0063289690273277245\n",
            "Loss at iteration 13000 = 0.0058908931238546725\n",
            "Loss at iteration 14000 = 0.005512608516006069\n",
            "Loss at iteration 15000 = 0.005182476872903816\n",
            "Loss at iteration 16000 = 0.0048917161980683915\n",
            "Loss at iteration 17000 = 0.004633572052779835\n",
            "Loss at iteration 18000 = 0.004402762163110134\n",
            "Loss at iteration 19000 = 0.004195094527461864\n",
            "Loss at iteration 20000 = 0.004007198822799868\n",
            "Loss at iteration 21000 = 0.0038363336794630803\n",
            "Loss at iteration 22000 = 0.003680245905651228\n",
            "Loss at iteration 23000 = 0.0035370659994072985\n",
            "Loss at iteration 24000 = 0.00340522946605881\n",
            "Loss at iteration 25000 = 0.0032834167865874245\n",
            "Loss at iteration 26000 = 0.0031705070658298883\n",
            "Loss at iteration 27000 = 0.0030655418500563497\n",
            "Loss at iteration 28000 = 0.0029676965979381696\n",
            "Loss at iteration 29000 = 0.0028762579769796267\n",
            "Loss at iteration 30000 = 0.0027906056406485627\n",
            "Loss at iteration 31000 = 0.0027101974853784325\n",
            "Loss at iteration 32000 = 0.0026345576345545535\n",
            "Loss at iteration 33000 = 0.0025632665774399715\n",
            "Loss at iteration 34000 = 0.002495953024344013\n",
            "Loss at iteration 35000 = 0.002432287138667125\n",
            "Loss at iteration 36000 = 0.002371974881152753\n",
            "Loss at iteration 37000 = 0.0023147532583533068\n",
            "Loss at iteration 38000 = 0.002260386310680053\n",
            "Loss at iteration 39000 = 0.002208661708846359\n",
            "Loss at iteration 40000 = 0.0021593878534922023\n",
            "Loss at iteration 41000 = 0.0021123913931030184\n",
            "Loss at iteration 42000 = 0.0020675150913417625\n",
            "Loss at iteration 43000 = 0.002024615987598966\n",
            "Loss at iteration 44000 = 0.001983563804678413\n",
            "Loss at iteration 45000 = 0.0019442395656443696\n",
            "Loss at iteration 46000 = 0.0019065343883924344\n",
            "Loss at iteration 47000 = 0.0018703484318010981\n",
            "Loss at iteration 48000 = 0.0018355899716331154\n",
            "Loss at iteration 49000 = 0.0018021745878824293\n",
            "Loss at iteration 50000 = 0.00177002444815953\n",
            "Loss at iteration 51000 = 0.0017390676740999614\n",
            "Loss at iteration 52000 = 0.0017092377797611879\n",
            "Loss at iteration 53000 = 0.0016804731726206938\n",
            "Loss at iteration 54000 = 0.0016527167091641348\n",
            "Loss at iteration 55000 = 0.0016259152982045601\n",
            "Loss at iteration 56000 = 0.0016000195460435169\n",
            "Loss at iteration 57000 = 0.001574983438401618\n",
            "Loss at iteration 58000 = 0.0015507640547385367\n",
            "Loss at iteration 59000 = 0.001527321311169869\n",
            "Loss at iteration 60000 = 0.0015046177286878177\n",
            "Loss at iteration 61000 = 0.0014826182238202128\n",
            "Loss at iteration 62000 = 0.0014612899192279565\n",
            "Loss at iteration 63000 = 0.0014406019720544698\n",
            "Loss at iteration 64000 = 0.0014205254181114805\n",
            "Loss at iteration 65000 = 0.0014010330302180987\n",
            "Loss at iteration 66000 = 0.0013820991892123087\n",
            "Loss at iteration 67000 = 0.0013636997663285084\n",
            "Loss at iteration 68000 = 0.001345812015786374\n",
            "Loss at iteration 69000 = 0.0013284144765698305\n",
            "Loss at iteration 70000 = 0.001311486882488484\n",
            "Loss at iteration 71000 = 0.0012950100797168213\n",
            "Loss at iteration 72000 = 0.0012789659510936179\n",
            "Loss at iteration 73000 = 0.001263337346541989\n",
            "Loss at iteration 74000 = 0.0012481080190393076\n",
            "Loss at iteration 75000 = 0.0012332625656258115\n",
            "Loss at iteration 76000 = 0.0012187863729942888\n",
            "Loss at iteration 77000 = 0.0012046655672500045\n",
            "Loss at iteration 78000 = 0.0011908869674717908\n",
            "Loss at iteration 79000 = 0.0011774380427420498\n",
            "Loss at iteration 80000 = 0.0011643068723465036\n",
            "Loss at iteration 81000 = 0.0011514821088732812\n",
            "Loss at iteration 82000 = 0.0011389529439675306\n",
            "Loss at iteration 83000 = 0.0011267090765204866\n",
            "Loss at iteration 84000 = 0.0011147406830930044\n",
            "Loss at iteration 85000 = 0.0011030383903922814\n",
            "Loss at iteration 86000 = 0.0010915932496366625\n",
            "Loss at iteration 87000 = 0.0010803967126589355\n",
            "Loss at iteration 88000 = 0.0010694406096118164\n",
            "Loss at iteration 89000 = 0.0010587171281511499\n",
            "Convergence Reached\n",
            "Final Loss at iteration 89191 = 0.0010566947753290299\n",
            "Training Complete for 0!\n",
            "\n",
            "Loss at iteration 0 = 3.831007583368594\n",
            "Loss at iteration 1000 = 0.5997842278447287\n",
            "Loss at iteration 2000 = 0.5817389619639577\n",
            "Loss at iteration 3000 = 0.5686973972063899\n",
            "Loss at iteration 4000 = 0.5591671447049075\n",
            "Loss at iteration 5000 = 0.5521052010585404\n",
            "Convergence Reached\n",
            "Final Loss at iteration 5365 = 0.5499924311387222\n",
            "Training Complete for 1!\n",
            "\n",
            "Loss at iteration 0 = 4.863656690626633\n",
            "Loss at iteration 1000 = 0.2784297328081489\n",
            "Loss at iteration 2000 = 0.22218842968824126\n",
            "Loss at iteration 3000 = 0.19100263391265385\n",
            "Loss at iteration 4000 = 0.17000828913967383\n",
            "Loss at iteration 5000 = 0.15468066684587287\n",
            "Loss at iteration 6000 = 0.14291726661955492\n",
            "Loss at iteration 7000 = 0.13356241312139652\n",
            "Loss at iteration 8000 = 0.12591857651829563\n",
            "Loss at iteration 9000 = 0.11953726185463817\n",
            "Loss at iteration 10000 = 0.11411609898942028\n",
            "Loss at iteration 11000 = 0.1094434251065298\n",
            "Loss at iteration 12000 = 0.1053664338517965\n",
            "Loss at iteration 13000 = 0.10177191778178667\n",
            "Loss at iteration 14000 = 0.09857413471844209\n",
            "Loss at iteration 15000 = 0.09570689386042322\n",
            "Loss at iteration 16000 = 0.0931182393582253\n",
            "Loss at iteration 17000 = 0.09076678609685897\n",
            "Loss at iteration 18000 = 0.08861913677848192\n",
            "Loss at iteration 19000 = 0.0866480245902137\n",
            "Loss at iteration 20000 = 0.08483095367880898\n",
            "Loss at iteration 21000 = 0.08314918799487196\n",
            "Loss at iteration 22000 = 0.08158698831492875\n",
            "Loss at iteration 23000 = 0.08013102894066448\n",
            "Loss at iteration 24000 = 0.07876994640466241\n",
            "Loss at iteration 25000 = 0.07749398646841793\n",
            "Loss at iteration 26000 = 0.07629472521439383\n",
            "Loss at iteration 27000 = 0.07516484662695527\n",
            "Loss at iteration 28000 = 0.0740979636927361\n",
            "Loss at iteration 29000 = 0.0730884733550279\n",
            "Loss at iteration 30000 = 0.072131438041495\n",
            "Loss at iteration 31000 = 0.07122248822592202\n",
            "Loss at iteration 32000 = 0.07035774177023912\n",
            "Loss at iteration 33000 = 0.06953373675176094\n",
            "Loss at iteration 34000 = 0.06874737520233576\n",
            "Loss at iteration 35000 = 0.06799587573435856\n",
            "Loss at iteration 36000 = 0.06727673344854602\n",
            "Loss at iteration 37000 = 0.06658768584259288\n",
            "Convergence Reached\n",
            "Final Loss at iteration 37423 = 0.06630477115657653\n",
            "Training Complete for 2!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.Get_Weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnXVPD-gbtGn",
        "outputId": "57bf901e-9739-4277-b35a-87c34fa54e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.45699917,  3.06357967, -4.22249984, -1.69097491,  0.99402015],\n",
              "       [ 0.24852084, -1.22906608,  0.25280425, -0.36975451,  0.97372035],\n",
              "       [-3.16073983, -3.57372245,  4.93967195,  4.83552388, -2.21799487]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with np.printoptions(precision=5, suppress = True):\n",
        "  print(model.Predict_Confidence(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlUf72sllTxB",
        "outputId": "a2639c95-d753-47e4-c1da-3c2ecb7f92a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00001 0.35969 0.98113]\n",
            " [0.99962 0.10922 0.     ]\n",
            " [0.      0.24057 0.97877]\n",
            " [0.99896 0.16359 0.     ]\n",
            " [0.00003 0.3375  0.62499]\n",
            " [0.00002 0.3488  0.66567]\n",
            " [0.9994  0.15053 0.     ]\n",
            " [0.00087 0.48619 0.01648]\n",
            " [0.      0.29332 0.9683 ]\n",
            " [0.00001 0.34899 0.9096 ]\n",
            " [0.00644 0.42708 0.00579]\n",
            " [0.99954 0.18433 0.     ]\n",
            " [0.00084 0.44113 0.00599]\n",
            " [0.      0.46109 0.89068]\n",
            " [0.0002  0.51107 0.1447 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.Predict_Class(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJz3Ck-2lj2q",
        "outputId": "2fe10ddf-2fdd-43b4-fc14-a8d12273f998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 2, 0, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(X, Y):\n",
        "  return (X == Y).sum() / len(Y) * 100\n",
        "\n",
        "predictions = model.Predict_Class(X_test)\n",
        "accuracy = calculate_accuracy(predictions, Y_test)\n",
        "print(f\"Accuracy = {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apU_SPgEYT1A",
        "outputId": "ebb31b89-50a1-49ef-c902-a052b0bcd321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 93.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9DPTvqQulJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}